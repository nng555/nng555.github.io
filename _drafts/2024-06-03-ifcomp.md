---
layout: post
title:  "Measuring Stochastic Data Complexity with Boltzmann Influence Functions"
categories: research conference blog
authors: <b>Nathan Ng</b>, Roger Grosse, Marzyeh Ghassemi
venue: ICML 2024
---

*This is part 2 of a blog post on our [**ICML 2024 paper**](https://arxiv.org/abs/2406.02745). This one goes into the method itself, while [**part 1**](/blog/2024/05/29/mdl.html) goes into the background on the Minimum Description Length Principle
and normalized maximum likelihood. I'd recommend reading that one first before this one unless you're already familiar with MDL.*

---

If you've been around the internet recently you might have seen posts showing Google's Gemini saying some [strange things](https://www.technologyreview.com/2024/05/31/1093019/why-are-googles-ai-overviews-results-so-bad/):

![Gemini says to eat rocks](/assets/img/google_rocks.jpg)<br>

Either I'm way behind on my daily rock intake or Gemini is making up information it knows nothing about.
If we want to be able to use these LLM systems (and in general any kind of AI system) in real world settings, we'll need better ways to tell when they're actually confident in their predictions, and when they're simply making things up.

A classical approach to this problem would use Bayesian principles to better quantify uncertainty, but these methods face significant scalability challenges.
In our paper we instead choose to take a **Minimum Description Length (MDL) Principle** based approach to quantifying uncertainty.

Specifically, for a given query example $x$, we want to calculate the pNML distribution which should give us better calibrated confidences and an easy way to measure data complexity.
In short, the pNML distribution considers every possible label $y \in \rY$ for $x$, and calculates the MLE for the **hindsight-optimal** model that can train on the original training set


## pNML and Stochastic Complexity for Deep Neural Networks

Unfortunately, attempting to calculate the pNML distribution or stochastic complexity for deep neural networks is not so simple.
There are 3 key issues we need to solve:

1. **The Infinity Problem**: Since

2. **Overconfidence on Training Data**

3. **Computational Efficiency**:
