---
layout: post
title:  "Measuring Stochastic Data Complexity with Boltzmann Influence Functions"
categories: research conference blog
authors: <b>Nathan Ng</b>, Roger Grosse, Marzyeh Ghassemi
venue: ICML 2024
tex: True
---

*This is part 2 of a blog post on our [**ICML 2024 paper**](). This one goes into the method itself, while [**part 1**](/blog/2024/05/29/mdl.html) goes into the background on the Minimum Description Length Principle
and normalized maximum likelihood. I'd recommend reading that one first before this one unless you're already familiar with MDL.*

---

If you've been around the internet recently you might have seen posts showing Google's Gemini saying some [strange things](https://www.technologyreview.com/2024/05/31/1093019/why-are-googles-ai-overviews-results-so-bad/):

![Gemini says to eat rocks](/assets/img/google_rocks.jpg)<br>

Either I'm way behind on my daily rock intake or Gemini is making up information it knows nothing about.
If we want to be able to use these LLM systems (and general any kind of AI system) in real world settings, we'll need better ways to tell when they're actually confident in their predictions, and when they're simply making things up.

A classical approach to this problem would use Bayesian principles to better quantify uncertainty, but these methods face significant scalability challenges.
In our paper we instead choose to take a **Minimum Description Length (MDL) Principle** based approach to produce calibrated output distributions.
Specifically, we'll show that we can use the pNML distribution to generated calibrated output distributions, and detect OOD test data and mislabeled training data by comparing the parametric and stochastic complexity.

This method benefits from not requiring defining any kind of prior (nor does it even depend on any notion of ground truth) and with some clever approximations, can be estimated accurately and efficiently even for deep neural networks.

## pNML for Deep Neural Networks

