---
layout: post
title:  "Measuring Stochastic Data Complexity with Boltzmann Influence Functions"
categories: research conference blog
authors: <b>Nathan Ng</b>, Roger Grosse, Marzyeh Ghassemi
venue: ICML 2024
---

*This is part 2 of a blog post on our [**ICML 2024 paper**](https://arxiv.org/abs/2406.02745). This one goes into the method itself, while [**part 1**](/blog/2024/05/29/mdl.html) goes into the background on the Minimum Description Length Principle
and normalized maximum likelihood. I'd recommend reading that one first before this one unless you're already familiar with MDL.*

---

$ 
  \def\X{\mathbf{X}}
  \def\Y{\mathbf{Y}}
  \def\rY{\mathcal{Y}}
  \def\loss{\mathcal{L}}
  \def\cost{\mathcal{J}}
  \def\hparam{\hat{\theta}}
  \def\oparam{\theta^{\star}}
$

If you've been around the internet recently you might have seen posts showing Google's Gemini saying some [strange things](https://www.technologyreview.com/2024/05/31/1093019/why-are-googles-ai-overviews-results-so-bad/):

![Gemini says to eat rocks](/assets/img/google_rocks.jpg)<br>

Either I'm way behind on my daily rock intake or Gemini is making up information it knows nothing about.
If we want to be able to use these LLM systems (and in general any kind of AI system) in real world settings, we'll need better ways to tell when they're actually confident in their predictions, and when they're simply making things up.

A classical approach to this problem would use Bayesian principles to better quantify uncertainty, but these methods face significant scalability challenges.
In our paper we instead choose to take a **Minimum Description Length (MDL) Principle** based approach to quantifying uncertainty.

Specifically, for a given query example $x$, we consider, for every possible label $y' \in \rY$, asking the question "How well could my model fit this additional training point $(x, y')$ along with the original training data?"
If any arbitrary label $y' \in \rY$ could be fit by the model, then its prediction for $x$ should be highly uncertain.
In contrast, if one label $y$ can be fit much easier than the others, then our prediction should have low uncertainty.

This notion is captured by the pNML distribution, which calculates the MLE for the original training data and the additional training point $(x, y')$, then renormalizes across all labels to achieve a valid probability distribution,
\\[ p_{\text{pNML}}(y' | x') = \frac{p_{\oparam(x', y')}(y' | x')}{\sum_{y \in \rY} p_{\oparam(x', y)}(y | x')} \\]
where $\oparam(x', y)$ is the MLE 

## pNML and Stochastic Complexity for Deep Neural Networks

Unfortunately, attempting to calculate the pNML distribution or stochastic complexity for overparameterized neural networks is quite difficult.
There are 3 key issues we need to solve:

1. **The Infinity Problem**: Since overparameterized neural networks are capable of memorizing their training data or any outlier data points, the hindsight-optimal model often simply memorizes the additional point $(x, y')$, incurring zero loss and causing our pNML distribution to degenerate into the uniform distribution.
This is called the infinity problem, when the pNML denominator is either constant (for discrete label spaces) or infinite (for continuous label spaces).
This makes the pNML useless for calibrating uncertainty or measuring complexity.

2. **Overconfidence on Training Data**: Even if we are able to solve the infinity problem, our original training data may restrict the learned model so much that it is unable to accomodate very low probability labels $y'$. 

3. **Computational Efficiency**:
