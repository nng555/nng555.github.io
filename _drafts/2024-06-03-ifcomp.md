---
layout: post
title:  "Measuring Stochastic Data Complexity with Boltzmann Influence Functions"
categories: research conference blog
authors: <b>Nathan Ng</b>, Roger Grosse, Marzyeh Ghassemi
venue: ICML 2024
tex: True
---

*This is part 2 of a blog post on our [ICML 2024 paper](). This one goes into the method itself, while [part 1](/blog/2024/05/29/mdl.html) goes into the background on the Minimum Description Length Principle
and normalized maximum likelihood. I'd recommend reading that one first before this one unless you're already familiar with MDL.*

---

If you've ever played around with any of the current LLM systems, you'll be familiar with their propensity to *hallucinate*, or produce seemingly correct but totally made up information.
Gemini has been particularly fond of doing so recently, and has been appropriately roasted on social media.
Here's a particularly egregious example:<br><br>

![Gemini says to eat rocks](/assets/img/google_rocks.jpg)<br>

Either I'm way behind on my daily rock intake or Gemini is making up information it knows nothing about.
If we want to be able to use these LLM systems (and general any kind of AI system) in real world settings, we'll need better ways to tell when they're actually confident in their predictions, and when they're simply making things up.

A classical approach to this problem would use Bayesian principles to better quantify uncertainty, but these methods face significant scalability challenges.
In our paper we instead choose to take a **Minimum Description Length (MDL) Principle** based approach to produce calibrated output distributions.
This method benefits from not requiring defining any kind of prior (nor does it even depend on any notion of ground truth) and with some clever approximations, can be estimated accurately and efficiently even for large neural networks. 
