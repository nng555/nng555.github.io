---
layout: post
title:  "Reproducing Kernel Hilbert Spaces"
categories: blog
authors: Nathan Ng
tex: True
---
While doing some background reading on positive definite kernels and Reproducing Kernel Hilbert Spaces (RKHS) I found that many definitions were a bit hard to understand intuitively.
This is my own attempt at explaining these concepts, mainly to solidify my own knowledge but also to share with others. 
$ 
  \def\bw{\mathbf{w}}
  \def\bx{\mathbf{x}}
  \def\by{\mathbf{y}}
  \def\bz{\mathbf{z}}
  \def\RR{\mathbb{R}}
  \def\H{\mathcal{H}}
  \def\wc{\mkern 2mu\cdot\mkern 2mu}
 $

## XOR: A Motivating Example
Linear model are a useful class of models that combine input dimensions linearly (i.e. via inner products) to compute an output.
One common model for performing binary classification is a Support Vector Machine (SVM), which attempts to learn the a separating hyperplane $ \bw^\intercal \bx = 0 $ parameterized by $ \bw $ between the two classes of data points.
Classification can then be performed on a test input $ \bx $ by computing $ \bw^\intercal \bx $ and thresholding at 0.
For convenience we assume the bias term has been absorbed into the input $ \bx $.

SVMs work well only when data is linearly separable, which is not always the case. Consider the problem of learning the XOR function, depicted below [^1]:

![XOR](/assets/img/rkhs/xor.png){: .half}

Visually, it is obvious that no line can separate these two classes in the original 2d input space. 
However, if we add a third dimension to our input feature vector, calculated as $ x_1 x_2 $, then the problem becomes simple!
To see why, notice that the only point with a non-zero third dimension value is $ (1, 1) $ which gets lifted above the other 3 points. Then a hyperplane can cut off this lifted point and the origin.

## Infinite-Dimensional Feature Spaces

The addition of this third dimension can be thought of as a function $ \phi(\cdot) $ that maps an input $ \bx $ to a new feature space. In our XOR example, our $ \phi $ maps:

\\[ \begin{bmatrix} x_1 \\\ x_2 \end{bmatrix} \mapsto \begin{bmatrix} x_1 \\\ x_2 \\\ x_1x_2 \end{bmatrix} \\]

In general many problems that are not linearly separable in the input feature space may be linearly separable in another feature space, as long as we can find the right function $ \phi $.
Well, if adding one monomial combination of our vector is good, why not consider an infinite-dimensional feature vector containing every single one? 
For example:
\\[ \begin{bmatrix} x_1 \\\ x_2 \end{bmatrix} \mapsto \begin{bmatrix} x_1 \\\ x_2 \\\ x_1^2 \\\ x_1x_2 \\\ x_2^2 \\\ x_1^3 \\\ x_1^2 x_2 \\\ \cdots  \end{bmatrix} \\]

<!--
<details>
  <summary><b><a>Infinite vectors as function spaces</a></b></summary>
  <div markdown="1">
We can generalize this notion of infinite-dimensional vector space as a function space instead, and define:
\\[ \phi: \RR^n \mapsto (\RR^n \mapsto \RR), \quad x \mapsto k(\wc, x) \\]
We call $ k(\wc, \wc) $ a **kernel** function. 
Combing back to our original example, 
to see how we can go from our set of infinite monomials to a specific kernel function, consider an ordering $  (i_1, i_2, \ldots)  $ of  all points in $  \RR^2  $. 
We can then define a function $  k(\wc, x)  $ such that 
\\[ \begin{align} k(i_1, x) &= x_1\\\ k(i_2, x) &= x_2\\\ k(i_3, x) &= x_1^2\\\ k(i_4, x) &= x_1 x_2 \\\ \ldots \end{align}\\]
and so on such that every monomial gets mapped to every point in $  \RR^2  $.
We have to be careful about the cardinality of these sets but luckily they are the same.
  </div>
</details>
-->

Of course now we have a problem. 
How are we going to calculate an infinite-dimensional inner product? 
Specifically, given some feature map $ \phi(\wc) $ defined by a kernel $ k(\wc, \wc) $ and two inputs $ \bx, \bx' $, we need to be able to calculate $ \langle \phi(\bx), \phi(\bx') \rangle $.

## Positive Definite Kernels

In order to admit an inner product, we need our kernel function $ k(\wc, \wc) $ to have some nice properties.
For a set of vectors $ (\bx_1, \bx_2, \ldots, \bx_m) \in \RR^n $, we define the Gram matrix as 
\\[ K := (k(\bx_i, \bx_j))_{ij} \\]
Then our kernel is **positive definite** if the matrix K is **positive semidefinite**[^2], or alternatively

\\[ \mathbf{c}^\intercal K \mathbf{c} = \sum_{i=1}^m \sum_{j=1}^m c_i c_j k(\bx_i, \bx_j) \geq 0, \quad \forall \mathbf{c} \in \RR^m \\]

This is an important property because it ensures that we can induce a norm on our function space which defines a Hilbert space.

<details>
  <summary><b><a>Examples of p.d. Kernels</a></b></summary>
  <div markdown="1">
Consider the dot product kernel $ k(\bx, \bx') = \bx^\intercal \bx'$
and the Euclidean distance kernel
$  k(\bx, \bx') = ||\bx - \bx'||^2  $.
The dot product kernel is positive definite, since 

\\[ \begin{align} 
\sum_{i=1}^m \sum_{j=1}^m c_i c_j \bx_i^\intercal \bx_j &= \sum_{i=1}^m \sum_{j=1}^m c_i c_j \sum_{a=1}^n x_{ia} x_{ja} \\\&= \sum_{i=1}^m \sum_{j=1}^m \sum_{a=1}^n c_i x_{ia} c_j x_{ja} \\\&= \sum_{a=1}^m \left( \sum_{i=1}^n c_i x_{ia} \right) \left( \sum_{j=1}^n c_j x_{ja} \right)\\\&=\sum_{i=1}^n \left( \sum_{i=1}^n c_i x_{ia} \right)^2 \geq 0
\end{align} \\]

However, the Euclidean kernel is not. Consider two 2d vectors $ \bx, \bx'  $, such that their Gram matrix is 
\\[ K = \begin{bmatrix} 0 & ||\bx - \bx'|| \\\ ||\bx' - \bx|| & 0 \end{bmatrix} \\]
If we choose $(1, -1)$ for our $\mathbf{c}$ values, then we get a negative value for $c^\intercal K c$ so the Gram matrix is not positive semidefinite.
  </div>
</details>

## The Reproducing Kernel Hilbert Space

Now that we have a positive definite kernel, let's define a dot product operator over the images of $\phi$.
Before we move onto the general case, let's go back to our original XOR example.
Remember that we had a feature map $\phi$ that transformed:
\\[ \begin{bmatrix} x_1 \\\ x_2 \end{bmatrix} \mapsto \begin{bmatrix} x_1 \\\ x_2 \\\ x_1x_2 \end{bmatrix} \\]
with the positive definite kernel 
\\[ k(\bx, \by) = \begin{bmatrix} x_1 \\\ x_2 \\\ x_1x_2 \end{bmatrix}^\intercal \begin{bmatrix} y_1 \\\ y_2 \\\ y_1y_2 \end{bmatrix} \\]

Now let's define a linear function of these features as $f(\bx) = ax_1 + bx_2 + cx_1x_2$, which maps from $\RR^2 \mapsto \RR$.
Since the function is linear, we can represent it nicely as a vector
\\[ f(\wc) = \begin{bmatrix} a \\\ b \\\ c \end{bmatrix} \\]
and evaluate $f(\bx)$ by taking the dot product 
\\[ f(\wc)^\intercal \phi(\bx) := \langle f(\wc), \phi(\bx) \rangle_\H = f(\bx) \\]
where $\H$ is the space of functions $\RR^2 \mapsto \RR$.

One interesting property of representing a function this way is that we can take $\phi$, which normally maps from $\RR^2 \mapsto \RR^3$, and instead use it as the *coefficients* to define another function!
To see how, let's take the another vector $\by$ with a feature vector as before:
\\[ \phi(\by) = \begin{bmatrix} y_1 \\\ y_2 \\\ y_1y_2 \end{bmatrix} \\]
and define a similar linear function $g(\by) = uy_1 + vy_2 + wy_1y_2$.
If we set $u = x_1, v = x_2, w = x_1x_2$, then the features from $\phi(x)$ are the coefficients for $g$ and define a function that maps from $\RR^2 \mapsto \RR$.
In other words, $\phi(x)$ can be used interchangeably as a function and a feature vector, and the function it defines is precisely our previously defined kernel:
\\[ \phi(\bx)(\wc) = k(\wc, \bx) \\]
We can write the pointwise evaluation of this function as $\langle k(\wc, \bx), \phi(\by) \rangle_\H$, and by symmetry, the evaluation of $\phi(\by) = k(\wc, \by)$ as $\langle k(\wc, \by), \phi(\bx) \rangle_\H$.
Combining them, we have the **reproducing property**
\\[ \langle k(\wc, \bx), k(\wc, \by) \rangle = k(\bx, \by) \\].

To generalize these ideas for the full RKHS, 






\\[ \langle f, g \rangle := \sum_{i=1}^m \sum_{j=1}^{m'} \bar{\alpha_i}\beta_j k(x_i, x_j) \\]


[^1]: Taken from Roger Grosse's <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc311_f21/lectures/lec05.pdf">Slides</a>
[^2]: Texts differ on whether to call this Gram matrix positive definite or positive semidefinite. Although it is a bit confusing, we call it positive semidefinite here since the definition uses the $\geq$ sign rather than the $>$ sign.
