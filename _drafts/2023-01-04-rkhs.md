---
layout: post
title:  "Reproducing Kernel Hilbert Spaces"
categories: blog
authors: Nathan Ng
tex: True
---

While doing some background reading on positive definite kernels and Reproducing Kernel Hilbert Spaces (RKHS) I found that there was a distinct lack of single explanation that stepped through the definitions in an intuitive and easy to understand way. 
This is my attempt at synthesizing my own personal understanding of RKHS. 
Where it aids in clarity for a nontechnical audience I have forgone mathematical rigor.

## A Motivating Example
The classic motivating example for the kernel trick and RKHS is attempting to learn a linear classifier for the XOR function.

SVMs and other linear models perform inner products of an input vector \\( x \in \mathbb{R}^n \\) with a weight vector \\( w \in \mathbb{R}^n \\).
However, we often want to learn to 

\\[ \langle f, g \rangle := \sum_{i=1}^m \sum_{j=1}^{m'} \bar{\alpha_i}\beta_j k(x_i, x_j) \\]
